"""Lessons log."""

# #### 04.02.2025
# Repository initialization, first commit and push

# #### 26.02.2025
# - Data processing workflow
# - Understanding Linear Regression
#   - One dimensional linear regression
#   - Best fit line
#   - Derivative definition
#   - Norms
#     - L1 norm, also known as the Manhattan norm
#     - L2 norm, also known as the Euclidean norm
#   - Smooth vs. Non-Smooth Functions
#   - Functional Loss
#   - Numerical method(iterations) in finding coefficients
#   - Definition of predictor(feature, independent) and target(dependent) variables
#   - Hypothesis function
#     - Liniarity
#     - Independence

# ### 05.03.2025
#
# - Road map for Linear Regression
#   - Introduction
#     - Definition
#     - Types of Linear Regressions
#       - Simple
#       - Multiple
#   - Basic definitions
#     - Variables
#       - target - dependent variable
#       - feature - independent variable
#     - Linear Regression Equation
#       - Simple
#       - Multiple
#     - Hypothesis
#   - Estimation of model parameters
#     - Ordinary Least Squares
#     - Gradient Descent
#   - Model quality estimation
#     - Metrics
#       - MAE (Mean Absolute Error)
#       - MSE (Mean Squared Error)
#       - RMSE (Root Mean Squared Error)
#       - R² (Коэффициент детерминации)
#   - Checking the Assumptions of Linear Regression
#     - Linearity of the relationship
#     - Normality of residuals
#     - Homoscedasticity
#     - Absence of multicollinearity between independent variables
#   - Problems and solutions
#     - Overfitting
#       - Regularization L1 and L2
#     - Underfitting
#       -  Add more features and use more complicated model
#     - Multicollinearity
#       - Removing correlated variables or using regularization (Ridge, Lasso)
#   - Regularization
#     - Ridge (L2)
#     - Lasso (L1)
#     - Elastic Net (L1 + L2)
#   - Advanced
#     - Multiple Linear Regression — with multiple predictors.
#     - Polynomial Regression — nonlinear relationships.
#     - Regularization (Ridge, Lasso, Elastic Net).
#     - Feature Scaling — StandardScaler, MinMaxScaler.
#     - Checking Assumptions using Residual Plots and QQ-Plots.
#
# - Machine learning
#   - Supervised
#     - Regression
#     - Classification
#   - Unsupervised
#     - Clustering
#
#
# - Classification of Regression Models
#
#     - Regression
#         - By Number of Factors
#             - Simple (Pairwise) Regression
#             - Multiple Regression
#         - By Relationship Type
#             - Linear
#             - Nonlinear
#                 - Linear by Variables
#                 - Linear by Parameters
#                 - Internally Nonlinear
#
#
# - Nonlinear functions
#   - Power Function Model – Used for dependencies with constant elasticity.
#   - Exponential Model – Used for processes with a constant growth rate.
#   - Logarithmic Model – Used for dependencies with a constant decrease in increment.
#   - Hyperbolic Model – Used for dependencies with a lower or upper limit.
#   - Polynomial Model of Different Degrees – Used for relationships with changing direction.
#
# - Steps for Building a Model
# - Probabilistic and non-probabilistic or deterministic approaches

# ### 12.03.2025
#
# - Time complexity and space for analytic and iterative approaches
# - Method of finding parameters:
#   - Analytic(Linear algebra)
#     - Pros/Con
#     - Main definition
#     - Equation
#   - Iterative(Calculus)
#     - Pros/Con
#     - Main definition
#     - Equation
# - Loss function and derivative
# - Calculate derivative
#   - Graphical approach
#   - Algebraic approach

# ### 19.03.2025
#
# - Concepts of Derivatives
#     - Definition of a Derivative
#     - Rules for Computing Derivatives
#       - Power Function
#       - Scalar
#       - Product Rule
#       - Quotient Rule
#       - Complex Function (Composition of Functions)
#

# ### 24.03.2025
#
# - Python practice
#   - Finding derivatives with the help of `sympy` module
#   - Optimization of a function and finding the minimum
#   - Building plots to represent decreasing loss
#
